{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install the repo and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip --quiet install git+https://github.com/wilson-labs/cola.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a4e6e",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe766bf",
   "metadata": {},
   "source": [
    "We now showcase some basic functionality present in CoLA. We'll start by showing how to define different types of Linear ops, then we'll show how to perform basic arithmetic with Linear ops and, finally, we'll conclude applying some linear algebra operations (like solves or log determinants) to the Linear ops.\n",
    "\n",
    "We'll work with torch.tensors in this example, but the same code can be run using JAX arrays (jnp.ndarrays). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a79e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f85c4233b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cola\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53243fb",
   "metadata": {},
   "source": [
    "## Creating a Linear Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb2c59",
   "metadata": {},
   "source": [
    "You can find several predefined Linear ops under cola.ops. We'll illustrate three basic cases: Dense, Diagonal and Tridiagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533e4e6",
   "metadata": {},
   "source": [
    "A Dense Linear Operator is nothing more than a wrapper on a dense matrix, where the wrapper defines a matmat function $v \\mapsto Av$ and holds several attributes such as dtype and shape.\n",
    "\n",
    "Let's start by defining a dense matrix and a vector to act upon. Below we show the entries of the matrix $A$, of the vector $v$ and the result of $Av$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b5a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[-0.2386, -1.0934,  0.1558,  0.1750],\n",
      "        [-0.9526, -0.5442,  1.1985,  0.9604],\n",
      "        [-1.1074, -0.8403, -0.0020,  0.2240],\n",
      "        [ 0.8766, -0.5379, -0.2994,  0.9785]])\n",
      "Av: tensor([-0.3842, -1.8328, -0.7573,  0.9795])\n"
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "A_dense = torch.randn(N, N)\n",
    "vec = torch.randn(N)\n",
    "print(f\"A: {A_dense}\\nAv: {A_dense @ vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e5e5f",
   "metadata": {},
   "source": [
    "To create a Dense operator simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca58aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cola.ops.operators.Dense'>\n"
     ]
    }
   ],
   "source": [
    "A = cola.ops.Dense(A_dense)\n",
    "print(type(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341fbcc",
   "metadata": {},
   "source": [
    "The previous operator now has a dtype and a shape attribute. More importantly, it can now act on the vector $v$ and get the same result as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d963cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype: torch.float32 | Shape: torch.Size([4, 4])\n",
      "tensor([-0.3842, -1.8328, -0.7573,  0.9795])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dtype: {A.dtype} | Shape: {A.shape}\")\n",
    "print(A @ vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc98c38",
   "metadata": {},
   "source": [
    "In order to make use of the benefits of CoLA, let's consider using some large structured Linear Operators. First let's start by creating some structured linear operators, such as a diagonal, a dense matrix, and a permutation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2dddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.,  1.,  0.],\n",
      "        [ 1., -2.,  1.],\n",
      "        [ 0.,  1., -2.]]), Type: Tridiagonal\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(300)\n",
    "upper = ones[:-1]\n",
    "lower = ones[:-1]\n",
    "diagonal= -2*ones\n",
    "\n",
    "T = cola.ops.Tridiagonal(lower, diagonal, upper)\n",
    "print(f\"{T[:3,:3].to_dense()}, Type: {T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6e4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4161, -0.2753,  1.9718,  0.0790, -0.1104]) tensor([-0.4161, -0.2753,  1.9718,  0.0790, -0.1104])\n"
     ]
    }
   ],
   "source": [
    "perm = torch.randperm(200)\n",
    "P = cola.ops.Permutation(perm)\n",
    "v = torch.randn(200)\n",
    "print((P @ v)[:5], v[perm[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926d60a",
   "metadata": {},
   "source": [
    "For these Linear Operators, the memory and compute cost for multiplication is only $O(n)$ rather than the $O(n^2)$ for dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ad4c5",
   "metadata": {},
   "source": [
    "## Doing binary operations with Linear ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b517daa",
   "metadata": {},
   "source": [
    "CoLA provides a similar interface to combine Linear ops as you would combine matrices. \n",
    "We can add and subtract (+,-), multiply by scalars (*,/), matrix multiply (@), transpose (.T), slice [:,:] and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a25ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  1.,  0.,  0.],\n",
      "        [ 1., -4.,  1.,  0.],\n",
      "        [ 0.,  1., -5.,  1.],\n",
      "        [ 0.,  0.,  1., -6.]])\n"
     ]
    }
   ],
   "source": [
    "D = cola.diag(torch.tensor([1, 2, 3, 4.]))\n",
    "DT = T[:4, :4] - D\n",
    "print(DT.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672753f0",
   "metadata": {},
   "source": [
    "Let's use these operations to make $B= A^T(T_{[:4,:4]}-D) + \\mu I$ regularized by $\\mu$ by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab53f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: DenseTridiagonal[slice(None, 4, None),slice(None, 4, None)]+-1.0diag(tensor([1., 2., 3., 4.]))+9.999999974752427e-07I\n",
      " tensor([[-0.2368,  2.4642,  5.4611, -6.3671],\n",
      "        [ 2.7361,  0.2431,  3.1194,  2.3872],\n",
      "        [ 0.7310, -4.6402,  0.9091,  1.7946],\n",
      "        [ 0.4354, -3.4425,  0.8188, -5.6472]])\n"
     ]
    }
   ],
   "source": [
    "mu = 1e-6\n",
    "B = A.T @ DT\n",
    "B += mu * cola.ops.I_like(B)\n",
    "print(f\"Object: {B}\\n {B.to_dense()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d92cf5",
   "metadata": {},
   "source": [
    "Linear Operators can also be combined in more involved ways, such as with `kron` for $\\otimes$, `block_diag`, and `concatenate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06defdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2386,  0.0000,  0.0000,  0.0000, -1.0934],\n",
      "        [ 0.0000, -0.4772,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.7157,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.9543,  0.0000],\n",
      "        [-0.9526,  0.0000,  0.0000,  0.0000, -0.5442]])\n",
      " shape: (16, 16), object: DenseâŠ—diag(tensor([1., 2., 3., 4.]))\n"
     ]
    }
   ],
   "source": [
    "K = cola.kron(A, D)\n",
    "print(f\"{K[:5,:5].to_dense()}\\n shape: {K.shape}, object: {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f660e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2100x2100 BlockDiag[cola.ops.operators.Kronecker[cola.ops.operators.Dense, cola.ops.operators.Permutation], cola.ops.operators.Tridiagonal, cola.ops.operators.Diagonal] with dtype=torch.float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 1 / (1 + torch.linspace(.2, 10, 1000))\n",
    "X = cola.block_diag(cola.kron(A, P), T, cola.diag(d))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fce179",
   "metadata": {},
   "source": [
    "Under the hood the operator $B$ is lazily defined and would know how to apply the Linear ops to any vector $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff6087",
   "metadata": {},
   "source": [
    "## Computing linear algebra operations (e.g. solves, logdet, diag, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bab0b",
   "metadata": {},
   "source": [
    "To solve the linear system $Bx=v$ we use the inverse function. This inverse function lazily defines $B^{-1}$ and hence applying it to $v$ yields the solution $x=B^{-1}v$. The inverse of $B$ is never computed, using $B^{-1}$ is simply how in CoLA we call linear solves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e803b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2100, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :5].to_dense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f86d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution error: 4.37e-06\n"
     ]
    }
   ],
   "source": [
    "X_inv = cola.inv(X)\n",
    "v = torch.randn(X.shape[-1])\n",
    "soln = X_inv @ v\n",
    "print(f\"Solution error: {torch.linalg.norm(X @ soln - v)/torch.linalg.norm(v):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d99cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA logdet: -1564.690, dense logdet -1564.690\n"
     ]
    }
   ],
   "source": [
    "print(f\"CoLA logdet: {cola.logdet(X):.3f}, dense logdet {torch.logdet(X.to_dense()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04d88f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA diag: tensor([0.0912, 0.0912, 0.0911, 0.0910, 0.0909])\n",
      "dense diag: tensor([0.0912, 0.0912, 0.0911, 0.0910, 0.0909])\n"
     ]
    }
   ],
   "source": [
    "print(f\"CoLA diag: {cola.diag(X)[-5:]}\\ndense diag: {torch.diag(X.to_dense())[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6432c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA exp(X): tensor([[0.3085, 0.2153, 0.0932, 0.0288, 0.0066],\n",
      "        [0.2153, 0.3085, 0.2152, 0.0930, 0.0275],\n",
      "        [0.0932, 0.2152, 0.3083, 0.2139, 0.0864],\n",
      "        [0.0288, 0.0930, 0.2139, 0.3016, 0.1865],\n",
      "        [0.0066, 0.0275, 0.0864, 0.1865, 0.2153]])\n",
      "scipy expm: [[0.30850834 0.21526882 0.09323522 0.02876082 0.0066488 ]\n",
      " [0.21526891 0.30850452 0.21523876 0.09302247 0.02746145]\n",
      " [0.09323531 0.21523881 0.30829167 0.21393949 0.08637363]\n",
      " [0.02876082 0.09302244 0.21393946 0.30164295 0.18647799]\n",
      " [0.0066488  0.02746145 0.08637364 0.18647805 0.21526927]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import expm\n",
    "scipy_expm = expm((T.to_dense()).numpy())\n",
    "print(f\"CoLA exp(X): {cola.exp(T)[-5:,-5:].to_dense().real}\\nscipy expm: {scipy_expm[-5:,-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e4504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA exp(X): tensor([[0.3085, 0.2153, 0.0932, 0.0288, 0.0066],\n",
      "        [0.2153, 0.3085, 0.2152, 0.0930, 0.0275],\n",
      "        [0.0932, 0.2152, 0.3083, 0.2139, 0.0864],\n",
      "        [0.0288, 0.0930, 0.2139, 0.3016, 0.1865],\n",
      "        [0.0066, 0.0275, 0.0864, 0.1865, 0.2153]])\n",
      "scipy expm: [[0.30850834 0.21526882 0.09323522 0.02876082 0.0066488 ]\n",
      " [0.21526891 0.30850452 0.21523876 0.09302247 0.02746145]\n",
      " [0.09323531 0.21523881 0.30829167 0.21393949 0.08637363]\n",
      " [0.02876082 0.09302244 0.21393946 0.30164295 0.18647799]\n",
      " [0.0066488  0.02746145 0.08637364 0.18647805 0.21526927]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"CoLA exp(X): {cola.exp(T, method='iterative')[-5:,-5:].to_dense().real}\\nscipy expm: {scipy_expm[-5:,-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ad8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = X.T @ X\n",
    "# let's annotate this matrix as positive definite to speed up the computation\n",
    "C = cola.PSD(C)\n",
    "e, v = cola.eig(C, num=3, which=\"SM\", max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0f3297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0953e-03, 1.0964e-02, 2.1969e-02, 4.2399e-02, 7.0133e-02, 1.0801e-01,\n",
      "        1.4944e-01, 1.9419e-01, 2.5152e-01, 3.1772e-01, 4.0139e-01, 4.7665e-01,\n",
      "        5.6189e-01, 6.4727e-01, 7.3868e-01, 8.8262e-01, 9.8502e-01, 1.1289e+00,\n",
      "        1.2921e+00, 1.4241e+00, 1.5924e+00, 1.7201e+00, 1.9020e+00, 2.0731e+00,\n",
      "        2.2735e+00, 2.3025e+00, 2.5105e+00, 2.8178e+00, 2.9081e+00, 3.2181e+00,\n",
      "        3.3807e+00, 3.6097e+00, 3.7622e+00, 3.9949e+00, 4.2446e+00, 4.4903e+00,\n",
      "        4.6545e+00, 4.9160e+00, 5.1424e+00, 5.3507e+00, 5.5901e+00, 5.7817e+00,\n",
      "        6.0946e+00, 6.3530e+00, 6.6405e+00, 6.8644e+00, 7.1608e+00, 7.3553e+00,\n",
      "        7.5980e+00, 7.7476e+00, 8.1347e+00, 8.3219e+00, 8.5606e+00, 8.8394e+00,\n",
      "        9.1587e+00, 9.3410e+00, 9.6295e+00, 9.8448e+00, 1.0079e+01, 1.0330e+01,\n",
      "        1.0445e+01, 1.0769e+01, 1.1044e+01, 1.1270e+01, 1.1530e+01, 1.1755e+01,\n",
      "        1.1908e+01, 1.2241e+01, 1.2330e+01, 1.2568e+01, 1.2806e+01, 1.2999e+01,\n",
      "        1.3167e+01, 1.3295e+01, 1.3514e+01, 1.3705e+01, 1.3877e+01, 1.4030e+01,\n",
      "        1.4265e+01, 1.4368e+01, 1.4547e+01, 1.4659e+01, 1.4789e+01, 1.4890e+01,\n",
      "        1.5074e+01, 1.5181e+01, 1.5324e+01, 1.5393e+01, 1.5457e+01, 1.5542e+01,\n",
      "        1.5648e+01, 1.5700e+01, 1.5777e+01, 1.5850e+01, 1.5878e+01, 1.5897e+01,\n",
      "        1.5960e+01, 1.5980e+01, 1.5989e+01, 1.5998e+01]) tensor([15.9922, 15.9965, 15.9991])\n"
     ]
    }
   ],
   "source": [
    "print(e, torch.linalg.eigh(C.to_dense())[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477eae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0163e-07,  7.4170e-07,  2.9827e-06,  6.1287e-06,  8.6353e-06,\n",
      "         1.5342e-05,  2.6599e-05,  4.8417e-05,  7.6746e-05,  1.1866e-04,\n",
      "         1.7418e-04,  2.4626e-04,  3.3815e-04,  4.5447e-04,  5.9902e-04,\n",
      "         7.7438e-04,  9.8632e-04,  1.2406e-03,  1.5368e-03,  1.8854e-03,\n",
      "         2.2899e-03,  2.7560e-03,  3.2899e-03,  3.8961e-03,  4.5839e-03,\n",
      "         5.3573e-03,  6.2235e-03,  7.1915e-03,  8.2694e-03,  8.2759e-03,\n",
      "         8.3232e-03,  8.3387e-03,  8.3862e-03,  8.4328e-03,  8.4770e-03,\n",
      "         8.5494e-03,  8.6306e-03,  8.6876e-03,  8.8099e-03,  8.8861e-03,\n",
      "         9.0070e-03,  9.1049e-03,  9.2122e-03,  9.3541e-03,  9.4663e-03,\n",
      "         9.6082e-03,  9.7719e-03,  9.8973e-03,  1.0062e-02,  1.0219e-02,\n",
      "         1.0413e-02,  1.0577e-02,  1.0742e-02,  1.0947e-02,  1.1148e-02,\n",
      "         1.1309e-02,  1.1519e-02,  1.1710e-02,  1.1989e-02,  1.2169e-02,\n",
      "         1.2379e-02,  1.2585e-02,  1.2774e-02,  1.3063e-02,  1.3274e-02,\n",
      "         1.3599e-02,  1.3839e-02,  1.4106e-02,  1.4308e-02,  1.4594e-02,\n",
      "         1.4847e-02,  1.5109e-02,  1.5435e-02,  1.5674e-02,  1.5950e-02,\n",
      "         1.6251e-02,  1.6572e-02,  1.6793e-02,  1.7180e-02,  1.7474e-02,\n",
      "         1.7737e-02,  1.8072e-02,  1.8349e-02,  1.8732e-02,  1.8965e-02,\n",
      "         1.9278e-02,  1.9682e-02,  2.0005e-02,  2.0330e-02,  2.0711e-02,\n",
      "         2.0968e-02,  2.1332e-02,  2.1810e-02,  2.2104e-02,  2.2535e-02,\n",
      "         2.2787e-02,  2.3144e-02,  2.3523e-02,  2.4008e-02,  2.4257e-02,\n",
      "         2.4710e-02,  2.5059e-02,  2.5514e-02,  2.5759e-02,  2.6320e-02,\n",
      "         2.6530e-02,  2.7130e-02,  2.7502e-02,  2.7702e-02,  2.8391e-02,\n",
      "         2.8645e-02,  2.9152e-02,  2.9520e-02,  3.0033e-02,  3.0466e-02,\n",
      "         3.0824e-02,  3.1343e-02,  3.1687e-02,  3.2150e-02,  3.2505e-02,\n",
      "         3.3102e-02,  3.3516e-02,  3.3864e-02,  3.4342e-02,  3.4821e-02,\n",
      "         3.5222e-02,  3.5721e-02,  3.6115e-02,  3.6796e-02,  3.7199e-02,\n",
      "         3.7696e-02,  3.8163e-02,  3.8597e-02,  3.8994e-02,  3.9525e-02,\n",
      "         4.0139e-02,  4.0617e-02,  4.1039e-02,  4.1563e-02,  4.1995e-02,\n",
      "         4.2415e-02,  4.2968e-02,  4.3487e-02,  4.4121e-02,  4.4444e-02,\n",
      "         4.5011e-02,  4.5485e-02,  4.6062e-02,  4.6439e-02,  4.6903e-02,\n",
      "         4.7780e-02,  4.8171e-02,  4.8567e-02,  4.9240e-02,  4.9480e-02,\n",
      "         5.0312e-02,  5.1090e-02,  5.1330e-02,  5.1776e-02,  5.2334e-02,\n",
      "         5.2884e-02,  5.3559e-02,  5.4153e-02,  5.4679e-02,  5.5177e-02,\n",
      "         5.5571e-02,  5.6362e-02,  5.6868e-02,  5.7325e-02,  5.7860e-02,\n",
      "         5.8304e-02,  5.9085e-02,  5.9607e-02,  6.0011e-02,  6.0638e-02,\n",
      "         6.1063e-02,  6.1680e-02,  6.2183e-02,  6.2871e-02,  6.3318e-02,\n",
      "         6.3649e-02,  6.4248e-02,  6.4970e-02,  6.5480e-02,  6.6254e-02,\n",
      "         6.7097e-02,  6.7510e-02,  6.7869e-02,  6.8581e-02,  6.9069e-02,\n",
      "         6.9298e-02,  7.0366e-02,  7.0793e-02,  7.1442e-02,  7.1839e-02,\n",
      "         7.2558e-02,  7.2970e-02,  7.3705e-02,  7.4182e-02,  7.4566e-02,\n",
      "         7.5457e-02,  7.6133e-02,  7.6512e-02,  7.6851e-02,  7.7812e-02,\n",
      "         7.8079e-02,  7.8718e-02,  7.9165e-02,  7.9635e-02,  8.0461e-02,\n",
      "         8.1001e-02,  8.1435e-02,  8.2248e-02,  8.2597e-02,  8.3153e-02,\n",
      "         8.3596e-02,  8.3758e-02,  8.4676e-02,  8.5164e-02,  8.5647e-02,\n",
      "         8.6182e-02,  8.6639e-02,  8.7142e-02,  8.7652e-02,  8.8161e-02,\n",
      "         8.8686e-02,  8.9107e-02,  8.9208e-02,  9.0241e-02,  9.0420e-02,\n",
      "         9.0784e-02,  9.1323e-02,  9.1867e-02,  9.2415e-02,  9.2969e-02,\n",
      "         9.3528e-02,  9.4091e-02,  9.4660e-02,  9.5235e-02,  9.5688e-02,\n",
      "         9.5813e-02,  9.6398e-02,  9.6987e-02,  9.7583e-02,  9.8184e-02,\n",
      "         9.8790e-02,  9.9403e-02,  1.0002e-01,  1.0064e-01,  1.0127e-01,\n",
      "         1.0191e-01,  1.0255e-01,  1.0277e-01,  1.0320e-01,  1.0385e-01,\n",
      "         1.0451e-01,  1.0518e-01,  1.0585e-01,  1.0653e-01,  1.0721e-01,\n",
      "         1.0791e-01,  1.0860e-01,  1.0931e-01,  1.1002e-01,  1.1023e-01,\n",
      "         1.1074e-01,  1.1147e-01,  1.1220e-01,  1.1294e-01,  1.1369e-01,\n",
      "         1.1445e-01,  1.1521e-01,  1.1598e-01,  1.1676e-01,  1.1755e-01,\n",
      "         1.1807e-01,  1.1834e-01,  1.1915e-01,  1.1996e-01,  1.2078e-01,\n",
      "         1.2160e-01,  1.2244e-01,  1.2328e-01,  1.2414e-01,  1.2500e-01,\n",
      "         1.2587e-01,  1.2631e-01,  1.2675e-01,  1.2764e-01,  1.2854e-01,\n",
      "         1.2945e-01,  1.3037e-01,  1.3130e-01,  1.3224e-01,  1.3319e-01,\n",
      "         1.3414e-01,  1.3496e-01,  1.3511e-01,  1.3609e-01,  1.3708e-01,\n",
      "         1.3808e-01,  1.3910e-01,  1.4012e-01,  1.4115e-01,  1.4220e-01,\n",
      "         1.4326e-01,  1.4403e-01,  1.4433e-01,  1.4541e-01,  1.4650e-01,\n",
      "         1.4761e-01,  1.4873e-01,  1.4986e-01,  1.5101e-01,  1.5216e-01,\n",
      "         1.5334e-01,  1.5354e-01,  1.5452e-01,  1.5572e-01,  1.5693e-01,\n",
      "         1.5816e-01,  1.5940e-01,  1.6066e-01,  1.6193e-01,  1.6321e-01,\n",
      "         1.6349e-01,  1.6451e-01,  1.6583e-01,  1.6716e-01,  1.6851e-01,\n",
      "         1.6988e-01,  1.7126e-01,  1.7266e-01,  1.7390e-01,  1.7408e-01,\n",
      "         1.7551e-01,  1.7696e-01,  1.7843e-01,  1.7992e-01,  1.8143e-01,\n",
      "         1.8295e-01,  1.8450e-01,  1.8477e-01,  1.8606e-01,  1.8765e-01,\n",
      "         1.8925e-01,  1.9088e-01,  1.9252e-01,  1.9394e-01,  1.9394e-01,\n",
      "         1.9394e-01,  1.9394e-01,  1.9394e-01,  1.9394e-01,  1.9394e-01,\n",
      "         1.9394e-01,  1.9394e-01,  1.9394e-01,  1.9394e-01,  1.9394e-01,\n",
      "         1.9395e-01,  1.9395e-01,  1.9395e-01,  1.9395e-01,  1.9419e-01,\n",
      "         1.9588e-01,  1.9613e-01,  1.9759e-01,  1.9933e-01,  2.0108e-01,\n",
      "         2.0287e-01,  2.0467e-01,  2.0650e-01,  2.0798e-01,  2.0835e-01,\n",
      "         2.1023e-01,  2.1214e-01,  2.1406e-01,  2.1602e-01,  2.1801e-01,\n",
      "         2.2002e-01,  2.2034e-01,  2.2205e-01,  2.2412e-01,  2.2622e-01,\n",
      "         2.2834e-01,  2.3050e-01,  2.3269e-01,  2.3321e-01,  2.3490e-01,\n",
      "         2.3715e-01,  2.3944e-01,  2.4175e-01,  2.4410e-01,  2.4648e-01,\n",
      "         2.4662e-01,  2.4890e-01,  2.5136e-01,  2.5385e-01,  2.5638e-01,\n",
      "         2.5894e-01,  2.6057e-01,  2.6155e-01,  2.6419e-01,  2.6688e-01,\n",
      "         2.6960e-01,  2.7237e-01,  2.7507e-01,  2.7518e-01,  2.7803e-01,\n",
      "         2.8093e-01,  2.8388e-01,  2.8687e-01,  2.8991e-01,  2.9013e-01,\n",
      "         2.9299e-01,  2.9613e-01,  2.9932e-01,  3.0255e-01,  3.0577e-01,\n",
      "         3.0585e-01,  3.0919e-01,  3.1259e-01,  3.1605e-01,  3.1957e-01,\n",
      "         3.2201e-01,  3.2314e-01,  3.2677e-01,  3.3047e-01,  3.3423e-01,\n",
      "         3.3805e-01,  3.3884e-01,  3.4194e-01,  3.4590e-01,  3.4992e-01,\n",
      "         3.5402e-01,  3.5629e-01,  3.5819e-01,  3.6243e-01,  3.6675e-01,\n",
      "         3.7115e-01,  3.7436e-01,  3.7563e-01,  3.8018e-01,  3.8483e-01,\n",
      "         3.8955e-01,  3.9307e-01,  3.9437e-01,  3.9927e-01,  4.0427e-01,\n",
      "         4.0936e-01,  4.1243e-01,  4.1455e-01,  4.1983e-01,  4.2522e-01,\n",
      "         4.3071e-01,  4.3245e-01,  4.3631e-01,  4.4202e-01,  4.4784e-01,\n",
      "         4.5314e-01,  4.5378e-01,  4.5984e-01,  4.6602e-01,  4.7233e-01,\n",
      "         4.7451e-01,  4.7876e-01,  4.8533e-01,  4.9203e-01,  4.9657e-01,\n",
      "         4.9887e-01,  5.0585e-01,  5.1299e-01,  5.1934e-01,  5.2027e-01,\n",
      "         5.2772e-01,  5.3532e-01,  5.4283e-01,  5.4309e-01,  5.5102e-01,\n",
      "         5.5914e-01,  5.6704e-01,  5.6743e-01,  5.7591e-01,  5.8458e-01,\n",
      "         5.9199e-01,  5.9345e-01,  6.0252e-01,  6.1181e-01,  6.1769e-01,\n",
      "         6.2130e-01,  6.3102e-01,  6.4098e-01,  6.4414e-01,  6.5116e-01,\n",
      "         6.6160e-01,  6.7137e-01,  6.7228e-01,  6.8323e-01,  6.9445e-01,\n",
      "         6.9937e-01,  7.2816e-01,  7.5775e-01,  7.8814e-01,  8.1936e-01,\n",
      "         8.5140e-01,  8.8427e-01,  9.1799e-01,  9.5257e-01,  9.8800e-01,\n",
      "         1.0243e+00,  1.0615e+00,  1.0995e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1289e+00,\n",
      "         1.1289e+00,  1.1289e+00,  1.1289e+00,  1.1385e+00,  1.1783e+00,\n",
      "         1.2191e+00,  1.2608e+00,  1.3034e+00,  1.3469e+00,  1.3913e+00,\n",
      "         1.4367e+00,  1.4830e+00,  1.5303e+00,  1.5785e+00,  1.6277e+00,\n",
      "         1.6778e+00,  1.7289e+00,  1.7809e+00,  1.8340e+00,  1.8880e+00,\n",
      "         1.9429e+00,  1.9989e+00,  2.0558e+00,  2.1137e+00,  2.1726e+00,\n",
      "         2.2324e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2838e+00,\n",
      "         2.2838e+00,  2.2838e+00,  2.2838e+00,  2.2933e+00,  2.3551e+00,\n",
      "         2.4179e+00,  2.4817e+00,  2.5464e+00,  2.6122e+00,  2.6789e+00,\n",
      "         2.7466e+00,  2.8152e+00,  2.8848e+00,  2.9554e+00,  3.0269e+00,\n",
      "         3.0994e+00,  3.1729e+00,  3.2473e+00,  3.3226e+00,  3.3989e+00,\n",
      "         3.4761e+00,  3.5542e+00,  3.6332e+00,  3.7132e+00,  3.7940e+00,\n",
      "         3.8757e+00,  3.8967e+00,  3.9584e+00,  4.0419e+00,  4.1262e+00,\n",
      "         4.2114e+00,  4.2975e+00,  4.3844e+00,  4.4722e+00,  4.5607e+00,\n",
      "         4.6501e+00,  4.7402e+00,  4.8311e+00,  4.9228e+00,  5.0153e+00,\n",
      "         5.1085e+00,  5.2024e+00,  5.2970e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,  5.3508e+00,\n",
      "         5.3924e+00,  5.4884e+00,  5.5851e+00,  5.6824e+00,  5.7804e+00,\n",
      "         5.8790e+00,  5.9782e+00,  6.0780e+00,  6.1784e+00,  6.2794e+00,\n",
      "         6.3809e+00,  6.4829e+00,  6.5854e+00,  6.6884e+00,  6.7918e+00,\n",
      "         6.8957e+00,  7.0001e+00,  7.1048e+00,  7.2099e+00,  7.3154e+00,\n",
      "         7.4213e+00,  7.5274e+00,  7.6339e+00,  7.7407e+00,  7.8477e+00,\n",
      "         7.9550e+00,  8.0625e+00,  8.1702e+00,  8.2780e+00,  8.3861e+00,\n",
      "         8.4942e+00,  8.6025e+00,  8.7108e+00,  8.8192e+00,  8.9277e+00,\n",
      "         9.0362e+00,  9.1446e+00,  9.2530e+00,  9.3614e+00,  9.4697e+00,\n",
      "         9.5779e+00,  9.6859e+00,  9.7939e+00,  9.9016e+00,  1.0009e+01,\n",
      "         1.0116e+01,  1.0223e+01,  1.0330e+01,  1.0437e+01,  1.0543e+01,\n",
      "         1.0649e+01,  1.0754e+01,  1.0859e+01,  1.0964e+01,  1.1068e+01,\n",
      "         1.1172e+01,  1.1275e+01,  1.1377e+01,  1.1480e+01,  1.1581e+01,\n",
      "         1.1682e+01,  1.1782e+01,  1.1882e+01,  1.1981e+01,  1.2079e+01,\n",
      "         1.2177e+01,  1.2274e+01,  1.2370e+01,  1.2465e+01,  1.2560e+01,\n",
      "         1.2653e+01,  1.2746e+01,  1.2838e+01,  1.2929e+01,  1.3019e+01,\n",
      "         1.3108e+01,  1.3196e+01,  1.3283e+01,  1.3369e+01,  1.3454e+01,\n",
      "         1.3538e+01,  1.3621e+01,  1.3703e+01,  1.3783e+01,  1.3863e+01,\n",
      "         1.3941e+01,  1.4018e+01,  1.4094e+01,  1.4169e+01,  1.4242e+01,\n",
      "         1.4314e+01,  1.4385e+01,  1.4455e+01,  1.4523e+01,  1.4590e+01,\n",
      "         1.4655e+01,  1.4719e+01,  1.4782e+01,  1.4843e+01,  1.4903e+01,\n",
      "         1.4962e+01,  1.5019e+01,  1.5074e+01,  1.5128e+01,  1.5181e+01,\n",
      "         1.5232e+01,  1.5281e+01,  1.5329e+01,  1.5375e+01,  1.5420e+01,\n",
      "         1.5463e+01,  1.5505e+01,  1.5544e+01,  1.5583e+01,  1.5620e+01,\n",
      "         1.5655e+01,  1.5688e+01,  1.5720e+01,  1.5750e+01,  1.5778e+01,\n",
      "         1.5805e+01,  1.5830e+01,  1.5853e+01,  1.5875e+01,  1.5895e+01,\n",
      "         1.5913e+01,  1.5930e+01,  1.5944e+01,  1.5957e+01,  1.5969e+01,\n",
      "         1.5978e+01,  1.5986e+01,  1.5992e+01,  1.5997e+01,  1.5999e+01]) tensor([-2.5722e-08,  5.0972e-07,  2.6101e-06])\n"
     ]
    }
   ],
   "source": [
    "e, v = cola.eig(C, num=3, which=\"LM\")\n",
    "print(e, torch.linalg.eigh(C.to_dense())[0][:3])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
