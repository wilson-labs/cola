{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install the repo and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip --quiet install git+https://github.com/wilson-labs/cola.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# CoLA Library Exercise\n",
    "\n",
    "This exercise will help you get familiar with the CoLA. \n",
    "\n",
    "## Installation\n",
    "You need a Python 3.10+ environment with either JAX or PyTorch installed (or both). You can install CoLA using pip as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/wilson-labs/cola.git\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, you can open the documentation in Colab and start working from there: [Quick Start](https://colab.research.google.com/github/wilson-labs/cola/blob/master/docs/notebooks/colabs/Quick_Start.ipynb)\n",
    "\n",
    "We strongly recommend that you read through the [documentation](https://cola.readthedocs.io/en/latest/index.html) to understand the library better.\n",
    "\n",
    "## Basic Exercises\n",
    "We'll start with some basic exercises to get you warmed-up for later. For each of the following, you can use either JAX or PyTorch. We recommend that you try both and see if you spot any difference on the behavior of CoLA.\n",
    "\n",
    "As explained in [Linear Operators: What and Why?](https://cola.readthedocs.io/en/latest/notebooks/LinOpIntro.html) `LinearOperators` are an efficient and scalable manner to represent matrices. To illustrate how to use the `LinearOperators` available in `CoLA`, I'm going to create a `Dense` `LinearOperator` using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense\n",
      "<class 'cola.ops.operators.Dense'>\n",
      "torch.float32\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import cola\n",
    "import torch\n",
    "\n",
    "dtype = torch.float32\n",
    "A = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=dtype)\n",
    "A_op = cola.ops.Dense(A)\n",
    "print(A_op)\n",
    "print(type(A_op))\n",
    "print(A_op.dtype)\n",
    "print(A_op.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the prints, `A_op` is no longer a `torch.tensor`!\n",
    "\n",
    "The `LinearOperator` class is quite simple as it only requires that we define three minimum requirements: (1) a `dtype`, (2) a `shape` and (3) a `matmul` function. In the dense case above, all these requirements are taken from the matrix $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(size=(A.shape[0],), dtype=dtype)\n",
    "print(A_op @ ones)\n",
    "print(A @ ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we get the same results and, notably, we can use the same syntax that we are accustomed to!\n",
    "You can find all the available operators [here](https://cola.readthedocs.io/en/latest/package/cola.ops.html).\n",
    "\n",
    "Now, create a diagonal `LinearOperator` using the `cola.ops.Diagonal` class, where the diagonal should be equal to $d=(-1, 2, 3)^T$ and a tridiagonal `LinearOperator` with a diagonal full of ones and the upper and lower bands being $\\alpha=(-1/2, 1/4)^T$. _Hint:_ Should $\\beta$ had the same shape as $\\alpha$?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal = torch.tensor(_, dtype=dtype)\n",
    "D_op = cola.ops.Diagonal(_)\n",
    "alpha = torch.tensor(_, dtype=dtype)\n",
    "beta = _\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dense version of each of these `LinearOperators` using the `to_dense()` method and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = _ \n",
    "T = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform some basic operations like addition, subtraction, and multiplication on the previous operators. Verify that the computations are correct using the dense API. I suggested one set of binary operations below, but please try more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_)\n",
    "print(A + D - T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the most popular linear algebra operation is solving a linear system, the so called \"solves\". You are now going to solve a linear system generated from a random matrix using CoLA. What function should you use from the [high level interface](https://cola.readthedocs.io/en/latest/package/cola.linalg.html)? Here is some code to create a random problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax.random import PRNGKey, normal, split\n",
    "\n",
    "N = _\n",
    "key = PRNGKey(seed=21)\n",
    "A = normal(key, shape=(N, N))\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a dense `LinearOperator` and find the solution. Compare it to the solution found using `JAX`'s own solver implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln = _\n",
    "soln_jax = jnp.linalg.solve(A, rhs)\n",
    "abs_diff = jnp.linalg.norm(_)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What floating point precision was used in the previous case? Is this relevant? Why or why not?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, CoLA dispatched a general linear solver as it did not have any extra information about the `LinearOperator` that it could exploit. Lets see how to add some information about the operator such as being PSD, self-adjoint or symmetric if real.\n",
    "In CoLA we do this using annotation operators like `cola.PSD`, `cola.SelfAdjoint` and `cola.SelfAdjoint`. Let's work with a PSD matrix now.\n",
    "Construct a `LinearOperator` $S = A A^T + \\mu I$. _Hint_: make $A$ dense, use `cola.ops.I_like` (see [docs](https://cola.readthedocs.io/en/latest/package/cola.ops.html#cola.ops.I_like)) and forget that you are using CoLA. Don't forget to annotate your operator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "N = 1_000\n",
    "key = PRNGKey(seed=21)\n",
    "dtype = jnp.float64\n",
    "A = cola.ops.Dense(normal(key, shape=(N, N)))\n",
    "mu = 1.e-1  # a large enough value ensures PSD\n",
    "S = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a PSD `LinearOperator` opens up the possibility in CoLA to dispatch our favorite algorithm: CG. As we all know, CG has a couple of hyperparameters like the tolerance (set it to $10^{-10}$) and the max number of iterations (set it to 10K). _Hint_: Pass `method`, `tol` and `max_iters` to the function that you used in the previous exercise. To see what is going on under the hood, take a look at the [source code](https://github.com/wilson-labs/cola/blob/main/cola/linalg/inv.py#L80)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln = _ @ rhs\n",
    "soln_jax = jnp.linalg.solve(S, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the difference much smaller that in the previous exercise?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the basic set of exercises lets focus on another fundamental linear algebra operation: eigendecomposition. For this case, use the $T$ matrix used here: \n",
    "[Linear Operators: What and Why?](https://github.com/wilson-labs/cola/blob/main/docs/notebooks/LinOpIntro.ipynb). Get the eigendecomposition of $T$ using double precision and compare it with PyTorch or JAX's implementation. _Hint_: Check CoLA's [API](https://cola.readthedocs.io/en/latest/package/cola.linalg.html) and maybe use an annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "dtype = _\n",
    "abs_diff = _\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are different algorithms being used? Is there a runtime benefit from dispatching a different algorithm?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Large Scale Machine Learning with CoLA\n",
    "\n",
    "Using JAX or PyTorch, pick any 3 out of the 5:\n",
    "\n",
    "### 1. GP\n",
    "\n",
    "GP Implement Gaussian Process (GP) inference with Radial Basis Function (RBF) kernel using `inverse()` from scratch on a dataset with at least 10k observations. You are not allowed to use GPyTorch. The formula for the GP posterior is:\n",
    "\n",
    "$$f_* | X, y, X_* \\sim \\mathcal{N}(\\mu_*, \\Sigma_*)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\mu_* = K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}y$$\n",
    "\n",
    "$$\\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}K(X, X_*)$$\n",
    "\n",
    "Here, $K$ is the RBF kernel, $X$ are the training inputs, $y$ are the training targets, $X_*$ are the test inputs, and $\\sigma^2_n$ is the noise variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O bike.mat \"https://www.andpotap.com/static/bike.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import os\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from scipy.io import loadmat\n",
    "import cola\n",
    "\n",
    "\n",
    "def load_uci_data(data_dir, dataset, train_p=0.75, test_p=0.15):\n",
    "    file_path = os.path.join(data_dir, dataset + '.mat')\n",
    "    data = np.array(loadmat(file_path)['data'])\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    X = X - X.min(0)[None]\n",
    "    X = 2.0 * (X / X.max(0)[None]) - 1.0\n",
    "    y -= y.mean()\n",
    "    y /= y.std()\n",
    "\n",
    "    train_n = int(floor(train_p * X.shape[0]))\n",
    "    valid_n = int(floor((1. - train_p - test_p) * X.shape[0]))\n",
    "\n",
    "    split = split_dataset(X, y, train_n, valid_n)\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = split\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "def split_dataset(x, y, train_n, valid_n):\n",
    "    train_x = x[:train_n, :]\n",
    "    train_y = y[:train_n]\n",
    "\n",
    "    valid_x = x[train_n:train_n + valid_n, :]\n",
    "    valid_y = y[train_n:train_n + valid_n]\n",
    "\n",
    "    test_x = x[train_n + valid_n:, :]\n",
    "    test_y = y[train_n + valid_n:]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "\n",
    "train_x, train_y, *_, test_x, test_y = load_uci_data(data_dir=\"./\", dataset=\"bike\")\n",
    "\n",
    "dtype = jnp.float32\n",
    "train_x, train_y = jnp.array(train_x, dtype=dtype), jnp.array(train_y, dtype=dtype)\n",
    "test_x, test_y = jnp.array(test_x, dtype=dtype), jnp.array(test_y, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the RBF kernel. Use the dim expansion trick below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rbf_cov(xi, xj):\n",
    "    xi, xj = jnp.expand_dims(xi, -2), jnp.expand_dims(xj, -3)\n",
    "    res = _\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I suggested some fixed hyperparameters. Finish computing the operators needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = jnp.array(100., dtype=dtype)\n",
    "noise = jnp.array(1., dtype=dtype)\n",
    "oscale = jnp.array(1., dtype=dtype)\n",
    "K_train_train = cola.ops.Dense(oscale * compute_rbf_cov(train_x / ls, _))\n",
    "K_test_train = _\n",
    "K_test_test = _\n",
    "K = cola.PSD(K_train_train + noise * _)\n",
    "mu = _\n",
    "Sigma = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the negative log-marginal likelihood for the kernel hyperparameters that you have. _Hint:_ You might want to use stochastic lanczos quadrature from CoLA's [algorithms](https://cola.readthedocs.io/en/latest/package/cola.algorithms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Hessian Spectrum\n",
    "Compute the eigenspectrum of the Hessian of a pretrained neural network. You can download weights of image classifiers pretrained on CIFAR10. Use `cola.algorithms.stochastic_lanczos_quadrature` and the spectral KDE smoothing method from [this paper](https://arxiv.org/pdf/1901.10159.pdf) (algorithm 1) to get a smoothed spectrum estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import stateless\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=False, num_workers=2)\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def flatten_params(params):\n",
    "    shapes = [p.shape for p in params]\n",
    "    flat_params = torch.cat([p.flatten() for p in params])\n",
    "    return flat_params, shapes\n",
    "\n",
    "\n",
    "def unflatten_params(flat_params, shapes):\n",
    "    params = []\n",
    "    i = 0\n",
    "    for shape in shapes:\n",
    "        size = torch.prod(torch.tensor(shape)).item()\n",
    "        param = flat_params[i:i + size]\n",
    "        param = param.view(shape)\n",
    "        params.append(param)\n",
    "        i += size\n",
    "    return params\n",
    "\n",
    "\n",
    "flat_p, shape = flatten_params(list(model.parameters()))\n",
    "flat_p = flat_p.detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "def stateless_model(fparams, x):\n",
    "    params = unflatten_params(fparams, shape)\n",
    "    names = list(n for n, _ in model.named_parameters())\n",
    "    nps = {n: p for n, p in zip(names, params)}\n",
    "    return stateless.functional_call(model, nps, x)\n",
    "\n",
    "\n",
    "def loss_fn(params):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    total_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        outputs = stateless_model(params, images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        total_loss += loss\n",
    "        if i > 10:\n",
    "            break  # For now we will only use a subset of the data\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "grads = torch.autograd.grad([loss_fn(flat_p)], flat_p, create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cola.ops.Hessian. or write your own LinearOperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLQ takes in a function to evaluate the nodes. What is the input shape of `x` below and what should be the output shape of `out` below? Is `fn` correctly defined?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x, mean=torch.tensor(0.), sigma=torch.tensor(1.)):\n",
    "    cons = 0.5 * torch.sqrt(2 * torch.pi * sigma)\n",
    "    out = cons * torch.exp(-0.5 * (x - mean) ** 2. / sigma)\n",
    "    return out\n",
    "\n",
    "\n",
    "out = cola.algorithms.stochastic_lanczos_quad(_)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Linear Regression\n",
    "Implement linear regression with a heteroscedastic noise model where $\\Phi$ is the design matrix, $\\beta$ are the parameters and $\\sigma_i$ is the measurement noise. The model is:\n",
    "\n",
    "$$y = \\Phi \\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, D)$$\n",
    "\n",
    "where $D$ is a diagonal matrix with $\\sigma_i^2$ on the diagonal. Add a Gaussian prior (regularization) if necessary. Hint: $\\hat{\\beta}_{MLE} = (\\Phi^T D^{-1} \\Phi)^{-1} \\Phi^T D^{-1} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_x, train_y, *_, test_x, test_y = load_uci_data(data_dir=\"./\", dataset=\"bike\")\n",
    "\n",
    "dtype = torch.float32\n",
    "train_x, train_y = torch.tensor(train_x, dtype=dtype), torch.tensor(train_y, dtype=dtype)\n",
    "test_x, test_y = torch.tensor(test_x, dtype=dtype), torch.tensor(test_y, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting you have to propose a function $\\sigma(x)$ which changes depending on the input. Usually, this $\\sigma(x)$ functions grow with the magnitude of $x$. What comes to mind? _Hint:_ don't forget that $\\sigma(x)$ should be a positive scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "sigma_x = sigma * _\n",
    "D = cola.ops.Diagonal(sigma_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\beta_{\\text{MLE}}$ don't forget to add some diagonal regularization ($\\mu$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = _\n",
    "mu = 0.05\n",
    "Phi_inv = _\n",
    "beta_mle = Phi_inv @ Phi.T @ cola.inverse(D) @ train_y\n",
    "print(beta_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the test RMSE. What about trying another functional form for $\\sigma(x)$? Did that change improve the Test RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = _\n",
    "print(f\"Test RMSE: {test_rmse:1.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement pagerank to find the most influential pages of Wikipedia.\n",
    " From the transition matrix on the [Linked- WikiText-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/), compute the largest eigenvector using `cola.eigmax`. From this eigenvector, rank the values to determine which web pages are most influential.\n",
    "\n",
    "The PageRank algorithm computes the stationary distribution of a Markov chain. Given a transition matrix $P$, the PageRank vector $r$ is the eigenvector corresponding to the largest eigenvalue (which should be 1 for a stochastic matrix).\n",
    "\n",
    "The transition matrix $P$ is defined as:\n",
    "\n",
    "$$P = (1-\\alpha)W + \\alpha \\mathbf{1}\\mathbf{1}^T$$\n",
    "\n",
    "where $W$ is the adjacency matrix normalized by the degree, $\\alpha$ is the damping factor (usually set to 0.15), and $\\mathbf{1}$ is a vector of ones.\n",
    "\n",
    "The adjacency matrix $A_{ij}$ is 1 if there is a link from page $i$ to page $j$ (not the other way around). The degree-normalized adjacency matrix $W$ is obtained by dividing each row of $A$ by its sum.\n",
    "\n",
    "The PageRank vector $r$ can be found by solving the eigenproblem:\n",
    "\n",
    "$$P^T r = r$$\n",
    "\n",
    "The entries of $r$ give the PageRank scores of the pages. The pages can then be ranked by these scores to find the most influential ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some starter code to create an adjacency matrix. The pages are in the form of Wikipedia QIDs. After finding the most popular QIDs, if they are not in the `page_to_title dict`, you can look them up using the wikipedia API with the `get_titles_from_wikidata` function.\n",
    "\n",
    "Suggestion: use the `cola.ops.Sparse` matrix for the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install io\n",
    "%pip install zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cola\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Request the zipped data\n",
    "link = \"https://rloganiv.github.io/linked-wikitext-2/static/media/linked-wikitext-2.142e2e52.zip\"\n",
    "r = requests.get(link)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "# Initialize a dictionary to hold the adjacency list\n",
    "adjacency_list = defaultdict(set)\n",
    "\n",
    "# Initialize a dictionary to map page ids to indices\n",
    "page_to_index, index_to_page, page_to_title, next_index = {}, {}, {}, 0\n",
    "\n",
    "# Extract the JSONL file\n",
    "files = ['valid.jsonl', 'train.jsonl', 'test.jsonl']\n",
    "for file in files:\n",
    "    with z.open(file) as f:\n",
    "        data = f.read().decode()\n",
    "        for line in data.splitlines():\n",
    "            data = json.loads(line)\n",
    "            current_page_id = data['annotations'][0]['id']\n",
    "\n",
    "            page_to_title[current_page_id] = data['title']\n",
    "            # If the current page id is not in the dictionary, add it\n",
    "            if current_page_id not in page_to_index:\n",
    "                page_to_index[current_page_id] = next_index\n",
    "                index_to_page[next_index] = current_page_id\n",
    "                next_index += 1\n",
    "\n",
    "            current_page_index = page_to_index[current_page_id]\n",
    "            for annotation in data['annotations']:\n",
    "                # If the annotation is a link to another page, add it to the adjacency list\n",
    "                if (annotation['source'] == 'WIKI') and (annotation['id'] != current_page_id):\n",
    "                    linked_page_id = annotation['id']\n",
    "\n",
    "                    # If the linked page id is not in the dictionary, add it\n",
    "                    if linked_page_id not in page_to_index:\n",
    "                        page_to_index[linked_page_id] = next_index\n",
    "                        index_to_page[next_index] = linked_page_id\n",
    "                        next_index += 1\n",
    "\n",
    "                    linked_page_index = page_to_index[linked_page_id]\n",
    "                    adjacency_list[current_page_index].add(linked_page_index)\n",
    "\n",
    "num_pages = len(page_to_index)\n",
    "adjacency_matrix = np.zeros((num_pages, num_pages), dtype=int)\n",
    "for page_index, linked_page_indices in adjacency_list.items():\n",
    "    for linked_page_index in linked_page_indices:\n",
    "        adjacency_matrix[page_index, linked_page_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the adjacency matrix into a torch tensor and get the max eigenvalue and eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "norm = _\n",
    "norm = torch.where(norm == 0., torch.tensor(1.), norm)\n",
    "W = cola.ops.Dense(adjacency_matrix / norm)\n",
    "alpha = 0.15\n",
    "P = _ + alpha * ones @ ones.T\n",
    "eigvec_max, eig_max = cola.eigmax(P.T)\n",
    "_, indices = torch.sort(eigvec_max, descending=True)\n",
    "print(indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running this with the sparse representation in CoLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_from_wikidata(qids):\n",
    "    qids_string = '|'.join(qids)\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': qids_string,\n",
    "        'format': 'json',\n",
    "        'props': 'labels',\n",
    "        'languages': 'en'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    titles = {}\n",
    "    for qid, entity in data['entities'].items():\n",
    "        if 'en' in entity['labels']:\n",
    "            titles[qid] = entity['labels']['en']['value']\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Make a pull request to CoLA.\n",
    " e.g., improvement to the documentation, new commonly used linear operator (e.g., Fisher information matrix, banded matrix, FFT matrix), bug fix. If your code for one of the above exercises is particularly clean, consider adding markdown text explaining the steps and let's add it to the CoLA documentation under examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
