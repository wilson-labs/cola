{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8548f7",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fd4cd",
   "metadata": {},
   "source": [
    "In this application, we'll show how `CoLA` can be used to perform Spectral Clustering. This application allows us to showcase our `Sparse` operator.\n",
    "\n",
    "In terms of the data, we will use the arXiv paper citation network of High Energy Physics. This is a directed graph showing the papers cited for each paper in the dataset. As in this case we will consider two papers to be related if at least one cited the other, then it will suffice to use an undirected graph. \n",
    "\n",
    "The following command would download the data in case it is not on the `data` folder under the user's `HOME` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e38602-bdc3-4fba-9fac-a5a1d4e24509",
   "metadata": {},
   "outputs": [],
   "source": [
    "![ ! -f \"$HOME/data/cit-HepPh.txt\" ] && wget -P ~/data https://www.andpotap.com/static/cit-HepPh.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fb57d0-38f9-424f-8707-66103b7f01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "input_path = os.path.join(os.environ['HOME'], \"data/cit-HepPh.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6a841-7295-4a17-a330-dc916586ab71",
   "metadata": {},
   "source": [
    "To pre-process the data we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78356d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_graph_data(filepath, dtype, xnp, num_edges=-1):\n",
    "    df = pd.read_csv(filepath, skiprows=4, delimiter=\"\\t\", header=None, names=[\"to\", \"from\"])\n",
    "    df = df[:num_edges]\n",
    "    df2 = pd.read_csv(filepath, skiprows=4, delimiter=\"\\t\", header=None, names=[\"from\", \"to\"])\n",
    "    df2 = df2[:num_edges]\n",
    "    df_undir = pd.concat((df, df2), axis=0)\n",
    "    df_undir = df_undir.drop_duplicates()\n",
    "    id_map = map_nodes_to_id(df_undir[\"from\"].unique())\n",
    "    N = len(id_map)\n",
    "    print(f\"Found {N:,d} nodes\")\n",
    "    for col in [\"from\", \"to\"]:\n",
    "        df_undir[col] = df_undir[col].map(id_map)\n",
    "    data = np.ones(shape=len(df_undir))\n",
    "    row, col = np.array(df_undir[\"to\"]), np.array(df_undir[\"from\"])\n",
    "    sparse_matrix = csr_matrix((data, (row, col)), shape=(N, N))\n",
    "    out = transform_to_csr(sparse_matrix, xnp, dtype)\n",
    "    data, col_ind, rowptr, shape = out\n",
    "    return data, col_ind, rowptr, shape\n",
    "\n",
    "\n",
    "def transform_to_csr(sparse_matrix, xnp, dtype):\n",
    "    data = xnp.array(sparse_matrix.data, dtype=dtype, device=None)\n",
    "    indices = xnp.array(sparse_matrix.indices, dtype=xnp.int64, device=None)\n",
    "    indptr = xnp.array(sparse_matrix.indptr, dtype=xnp.int64, device=None)\n",
    "    return data, indices, indptr, sparse_matrix.shape\n",
    "\n",
    "\n",
    "def map_nodes_to_id(nodes):\n",
    "    out = {}\n",
    "    for idx in range(len(nodes)):\n",
    "        out[int(nodes[idx])] = idx\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0badcdf",
   "metadata": {},
   "source": [
    "The function `load_graph_data` creates the column indices and row pointers needed for the sparse [CSR format](https://en.wikipedia.org/wiki/Sparse_matrix). Now, we can load the data and create our `Sparse` adjacency matrix as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5f6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34,545 nodes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cola\n",
    "from cola.backends import torch_fns as xnp\n",
    "\n",
    "num_edges = -1\n",
    "dtype = torch.float64\n",
    "data, col_ind, rowptr, shape = load_graph_data(input_path, dtype, xnp, num_edges)\n",
    "Ad = cola.ops.Sparse(data, col_ind, rowptr, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e37db8",
   "metadata": {},
   "source": [
    "Given the adjacency matrix, we can now create the normalized Laplacian defined as $L=I - D^{-1/2} A D^{-1/2}$, where $D$ is the diagonal matrix that contains the degree of each node, $A$ is the adjacency matrix and $I$ is the identity. We can create the Laplacian operator really easily in `CoLA` as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8918cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the Laplacian: (34545, 34545)\n"
     ]
    }
   ],
   "source": [
    "Deg = cola.ops.Diagonal(Ad @ torch.ones((Ad.shape[0], ), dtype=dtype))\n",
    "Id = cola.ops.I_like(Deg)\n",
    "Lap = Id - cola.inv(cola.sqrt(Deg)) @ Ad @ cola.inv(cola.sqrt(Deg))\n",
    "Lap = cola.SelfAdjoint(Lap)\n",
    "print(f\"Size of the Laplacian: {Lap.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedbd96",
   "metadata": {},
   "source": [
    "Where I added the `SelfAdjoint` annotation at the end to ensure that `CoLA` dispatches algorithms for this type of symmetric operator. Spectral Clustering requires that we compute the eigenvectors of the smallest eigenvalues and use those eigenvectors as an embedding of our data. Once we do this, we can then use k-means to cluster points nearby as those points are related to cliques in the original graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89dac75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Non keyed randn used. To be deprecated soon.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embedding_size, n_clusters = 8, 8\n",
    "eigvals, eigvecs = cola.eig(Lap, k=Lap.shape[0], which=\"LM\", alg=cola.Lanczos(max_iters=300))\n",
    "x_emb = eigvecs[:, :embedding_size].to_dense()\n",
    "kmeans = KMeans(n_clusters=n_clusters).fit(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ec3f5b-219a-4da2-bd5e-ee0b712ca21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 7, 0, 7, 0, 7, 0, 0, 7, 7, 0, 0, 0,\n",
       "       0, 0, 7, 7, 7, 0, 7, 0, 0, 7, 7, 0, 0, 0, 7, 0, 0, 7, 7, 7, 7, 0,\n",
       "       0, 7, 0, 7, 7, 0, 7, 7, 7, 0, 0, 0, 7, 7, 0, 0, 0, 0, 7, 7, 7, 7,\n",
       "       7, 7, 0, 0, 7, 7, 0, 0, 7, 7, 7, 0, 0, 0, 0, 7, 0, 7, 7, 7, 7, 7,\n",
       "       0, 0, 7, 7, 0, 0, 0, 7, 0, 7, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_[100:200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
